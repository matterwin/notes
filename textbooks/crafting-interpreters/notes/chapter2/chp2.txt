# Chapter 2

- language is like a mountain: very climb to more high-level representation, then descend to lower-level rep for the CPU to understand and execute


- src code -> scanning -> tokens -> parsing -> syntax tree -> semantic analysis -> intermediate representation(s) -> code generation -> machine code


- first step: Scanning (Lexical Analysis)
A scanner (or lexer) takes in stream of characters and chunks them into tokens.
Token examples: numbers (123), string literals ("hi"), identifiers (min).
Lexer usually discards comments and whitespaces to create a clean stream of tokens.


- second step: Parsing (Parse Tree)
Consists of grammar (or grammar rules), which makes larger components built by tokens.
Parser takes a sequence of tokens & builds a tree structure that mirrors the grammar.
Common names of structures: parse tree, syntax tree, abstract syntax tree (AST's).
Reports syntax errors to user.


- third step: static analysis (or semantic analysis)
Scanning and parsing are similar across all implementations; here is when lang-spec comes into play.
At this point, we know the syntactical structure of the code, but nothing else.
Example: x + y
We know we are adding x and y, but what do those variables refer to? are they global, private, public? where were they defined?

- Binding or resolution
For each identifer, we find where that name was defined and wire the two together and we get the scope of it.

If the lang is statically typed, here we type check.
We can figure out the types of x & y, and see if their types don't support being added together, if not we report a type error.

Here are a few places to store all this semantic information (your choice):

- Attributes
Info gets stored right back on AST & are extra fields in the nodes that aren't initialized during parsing but now get filled in later.

- Symbol Table
A look-up table where typically the keys to this table are identifiers (names of variables and declarations).
The values of the key tell us what the identifer refers to.

- Most powerful tool is to transform the tree entirely into a new datastructure that more directly reps the semantics of the code -- intermediate code representation


All this is called the frontend of the compiler, next section is the "middle end".


- fourth step: intermediate representations
front end of compiler is specific to the source language the program is written in.
back end is concerned with the final architecture where the program will run.
middle end the code isn't tightly tied to either front or back end (shortname -- IR).

IR acts as an interface between the two.
This allows you to support multiple source languages and target platforms with less effort.

Example: you want to implement Pascal, C, Fortran compilers and want to target x86, ARM, SPARC.
Normally, that means you would write 9 full compilers for each lang and target.

A shared IR reduces that dramatically.
You write 1 frontend for each lang, obviously b/c each lang is different.
The same for 1 backend for each target architecture.
Now you can mix and match those to get the combinations you want through IR.

Optimiziations can also come alive from IR.


- fifth step: optimization
once we understand what a program does, we are able to swap it out with a different program that has the same semantics but implements them more efficiently (optimization)

- constant floating
if some expression evaluates to the exact same value, we can do the evaluation at compile time and replace the code for the expression with its result.

Example:
pennyArea = 3.14159 * (0.75 / 2) * (0.75 / 2);

We can do all of that arithmetic in the compiler and change the code to:
pennyArea = 0.4417860938;

Many successful langs have few compile-time opts, but more a focus on run-time opts: CPython and Lua

Optimization keywords for your use:
“constant propagation”, 
“common subexpression elimination”, 
“loop invariant code motion”, 
“global value numbering”,
“strength reduction”, 
“scalar replacement of aggregates”, 
“dead code elimination”, 
and “loop unrolling”.


- sixth step: code generation
Last step is converting to a form that a machine can actually run.
Generating code that is now a primitive assembly-like instructions a CPU can run.
In back end portion of compiler, like evolution in reverse, more and more primitive.

We have a decision to make:
do we generate instructions for a real CPU or a virtual one?

If we gen real machine code, we get an executable that the OS can load directly onto the chip.
It's lightning fast, but a lot of work.
Also, your compiler is then tied to a specific architecture, i.e. if you target x86, your compiler won't run on ARM.

To get around that, utilize virtual machine code.

- Bytecode (or p-code)
Hypothetical, idealized, portabile machine code that each instruction is often a byte long (hence the name)

These synthetic instructions are designed to map a little more closely to the
language’s semantics, and not be so tied to the peculiarities of any one computer
architecture and its accumulated historical cruft. You can think of it like a
dense, binary encoding of the language’s low-level operations.


- seventh step: virtual machine
if your compiler produces bytecode, your work isn't over yet.
You have 2 options:

1. write a mini-compiler for each target architecture that converts bytecode to native code for that machine.
So, you still need to do work for each chip you support, but this last stage is pretty simple and you can reuse the rest of the compiler pipeline across all of the machines you support.
Basically, your using bytecode as IR.


2. Or write a virtual machine (VM), a program that emulates a hypothetical chip supporting your virtual architecture at runtime. 
However, running bytecode in a VM is slower than translating it to native code ahead of time b/c every instruction must be simulated at runtime each time it executes.
In return, you get simplicity and portability. 
Implement your VM, in, say, C, and you can run your language on any platform that has a C compiler.
Our second interpreter works like this.

Side note:
 A system
virtual machine emulates an entire
hardware platform and operating system in
software. This is how you can play Windows
games on your Linux machine, and how
cloud providers give customers the user
experience of controlling their own “server”
without needing to physically allocate
separate computers for each user

The kind of VMs we’ll talk about in this book
are language virtual machines or process
virtual machines if you want to be
unambiguous.


How java works:
java src code is in .java files.

javac compiles src code into bytecode, which is platform-independent, intermediate representation of the code.

the bytecode is stored in the .class files, which is produced after compiling

bytecode is a low-level set of instructions that is not directly executed by the CPU.

instead, its designed to be executed by the JVM (java virtual machine)

the JVM interprets or compiles (Just-in-time compliation) the bytecode into machine code for the host machine's architecture.

THE JVM acts as an intermediary layer, providing a consistent execution env across different platforms.

When the compiled file is ran, java fileName, the JVM loads the fileName.class file, interprets the bytecode, and executes the program, outputting whatever like "Hello World".

The JVM is responsible for providing runtime functionalities like memory management, exception handling, JIT compilation, class loading, etc.


- eighth step: runtime
last step is running the code now.

If we compiled it to machine code, we tell the OS to load the executable and off it goes.
If we compiled it to bytecode, we need to start up the VM and load the program into that.

In both cases, we need some services that the lang itself provides like instanceof (we need types of each object during execution) or a garbage collector to manage unused bits.

All of this happens at runtime.

In a fully compiled language, the code implementing the runtime gets inserted directly into the resulting executable.
In, say, Go, each compiled application has
its own copy of Go’s runtime directly embedded in it.

If the lang is run inside an interpreter or VM, then the runtime lives there.
This is how most implementations of languages like Java, Python, and JavaScript work.






# 2.2 Shortcuts and Alternate Routes

- Single-pass compilers
restrict the design of the language by interleaving parsing, analysis, code gen, so that they produce output code directly in the parser, without every allocation to any syntax trees or other IRs.

there is no intermediate data structure to store global information about the program, and you dont need to revisit previously parsed part of the code.

which means as soon as you see some expression, you need to know enough to correctly compile it.

Before, memory was very precious and compilers could not hold an entire source file.
Which is why C you can't call a function above the code that defines it unless you have an explicit forward declaration that tells the compiler what it needs to know to generate code for a call to the later function.

Pascal was designed with this single-pass compiler design in mind: Pascal's grammar requires type declarations to appear first in a block



- Tree-walk interpreters
some programming langs begin executing code right after parsing it to an AST (with bit of static analysis applied).
To run the program, the interpreter traverses the syntax tree one branch and leaf at a time, evaluating each node as it goes.

This is slow, which is why general-purpose langs don't do this and student projects do.

Our first interpreter will roll this way, and it is called a "tree-walk interpreter".

Early versions of Ruby were tree-walkers, but later switched to YARV "Yet Another Ruby VM" and is a bytecode VM.



- Transpilers
transpilers, short for "source-to-source compilers," are tools that take source code written in one programming language and convert it into equivalent source code in another programming language.

Unlike traditional compilers that translate source code into machine code or bytecode, transpilers focus on converting code between high-level programming languages.

Typescript transpiles to javascript to run in the web as an example.
And back in the day JS used to be the only way to execute code in a browser, but other tools like Web Assembly came out as an alternative, but almost every lang has a compiler that targets JS to get your code running in a browser.

First transpiler, XLT86, translated 8080 assembly into 8066 assembly.
8080 was an 8-bit chip and the latter was a 16-bit chip.

With UNIX machines being so prominent, a long tradition of compilers produced C as their output language.
Targeting C was good b/c it is fast and a good way to get your language running on a lot of architectures.

The front end - scanner and parser - of a transpiler looks like other compilers.

If the src lang is a simple syntactic skin over the target lang, it may skip analysis entirely and go straight to outputting code in the destination lang.

If the two langs are more semantically different, then you'll see more typical phases of a full compiler like analysis and even optimization. Then, when it's time fore code generation, instead of outputting binary language like machine code, you produce a string of grammatically correct source (well, destination) code in the target language.

Either way, you can run that resulting code through the output language's exisiting compilation pipeline and you're good to go.



- Just-in-time compilation (JIT)
less of a shortcut (ex JVM)
The fastest way to execute code is by compiling it to machine code, but you might not know what architecture your end user's machine supports.

What to do?
You can do the same thing that the HotSpot JVM, Microsoft's CLR and most JavaScript interpreters do.
CLR is Common Language Runtime and is apart of MS's .NET framework.
JVM is Java Virtual Machine for .class files

On the end user's machine, when the program is loaded -- either from source in the case of JS, or platform-independent bytecode for the JVM and CLR -- you compile it to native for the architecture their computer supports.

The most sophisticated JITs insert profiling hooks into the generated code to
see which regions are most performance critical and what kind of data is
flowing through them. Then, over time, they will automatically recompile those
hot spots with more advanced optimizations. (i.e. where HotSpot gets its name for JVM)






# 2.3 Compilers and Interpreters


"What's the difference between a compiler and an interpreter?"

- Compiling
is an implementation technique that involves translating a source language to some other -- usually lower-level -- form.
When you generate bytecode or machine code, you are compiling.
When you transpile to another high-level language you are compiling too.

When a language implementation "is a compiler", we mean it translates source code to some other form but doesn't execute it.
The user has to take the resulting output and run it themselves.

Conversely, when a language implementation "is an interpreter", we mean it takes in source code and executs it immediately.
It runs programs "from source".

GCC and Clang take your C code and compile it to machine code.
An end user runs that executable directly and may never even know which tool
was used to compile it. So those are compilers for C.

What of CPython?
When your run a Python program using CPython, the code is parse and converted to an internal bytecode format, which is then executed inside the VM.
From the user's perspective, this is clearly an interpreter -- they run their program from source.
But underneath CPython's skin, they is compiling.

So here CPython is an interpreter and it has a compiler.
Most scripting languages work this way.


Compiler: 
javac
GCC
Typescript
CoffeeScript
Rust 
clang

Both: (compiler & interpreter)
C#
Haskell
CPython
YARV (Ruby)
Guile (Scheme)
V8 (JS)
PHP4
Scala 
Lua 
clox 
Go 

Interpreter:
MRI (Ruby)
jlox
PHP3

The Go tool is even more of a horticultural
curiosity. If you run go build, it compiles
your Go source code to machine code and
stops. If you type go run, it does that then
immediately executes the generated
executable.

So go is a compiler (you can use it as a tool to
compile code without running it), is an
interpreter (you can invoke it to immediately
run a program from source), and also has a
compiler (when you use it as an interpreter, it
is still compiling internally).


The 2nd interpreter we'll do is in that overlapping region, since it internally compiles to bytecode.






# Challenges

Just-in-time compilation tends to be the fastest way to implement a dynamicallytyped language, but not all of them use it. What reasons are there to not JIT?

Ans: JIT is complex and requires expertise in its development b/c of its benefits for dynamically-typed langs translating code to machine code at runtime.



Most Lisp implementations that compile to C also contain an interpreter that lets
them execute Lisp code on the fly as well. Why?

Ans: Lisp can be used for interactive development, debugging capabilities, and platform independence




